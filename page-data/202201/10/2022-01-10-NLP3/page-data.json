{
    "componentChunkName": "component---src-templates-post-template-tsx",
    "path": "/202201/10/2022-01-10-NLP3/",
    "result": {"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"<h3>Optimization methods</h3>\n<ul>\n<li>손실을 최소화하는 최적화 방법을 배우기 위해 알아야 하는 것 두 가지.</li>\n</ul>\n<ol>\n<li>최적화 방법 선택하기</li>\n<li>파라미터를 평가하기 위한 최적화 알고리즘 사용 방법.</li>\n</ol>\n<ul>\n<li>\n<p>Random guided search<br>\nidea : 랜덤하게 파라미터를 수정하면서 최소화 하는 방향으로 간다.<br>\n어떤 NN이나 손실함수에도 적용하기 쉽지만, 고차원 파라미터 공간에서는 차원의 저주(curse of dimensionality)로 비효율적이다.</p>\n</li>\n<li>\n<p>Gradient Based Algorithm<br>\n이 알고리즘은 손실함수가 연속이고(continuous) 미분가능한(differentiable) 성질을 갖고 있어야 한다.<br>\nidea : 가장 먼저 파라미터를 랜덤픽하고, 손실함수를 미분하여 기울기를 구한다. 기울기 값을 파라미터 값에서 빼주면, 손실함수가 최소가되는 파라미터값에 가까워진다.<br>\n단, 기울기 값을 파라미터 값에서 뺄 때, 학습률(learning rate)를 고려하여 빼준다. 학습률이 낮으면 오랜 시간이 걸리고, 학습률이 크면 변동 폭이 커서 파라미터의 값을 제대로 찾지 못하게 된다.</p>\n</li>\n</ul>\n<h3>Backpropagation</h3>\n<ul>\n<li>자동 미분법(Automatic differentiation)<br>\n규모가 굉장히 큰 방정식의 기울기를 어떻게 구할 것인가에 대한 문제 → 자동 미분법\n모델을 연결하는 DAG는 미분 가능한 함수들로 구성되어 있으며, 미분의 연쇄법칙으로 자동 미분됨.</li>\n<li>Backward Computation - 역전파(Backpropagation)<br>\n과정 : 각 노드에서 Jacobian-vector product 계산 → DAG 역순으로 전파<br>\n장점 : 기울기를 직접 계산할 필요가 없으며, 모델을 만드는데 집중할 수 있다.</li>\n</ul>\n<h3>Gradient-Based Optimization</h3>\n<p>우리가 역전파 알고리즘으로 패키지에서 사용하는 것은 Gradient descent, L-BFGS, Conjugate gradient가 있는데, 매개변수가 많을수록 시간이 오래걸린다. 이유는 훈련 샘플 전체의 Loss는 각 샘플에 대한 Loss 합이며 데이터가 많아질수록 시간이 오래걸리기 때문이다. 따라서 off-the shelf gradient-based optimization을 사용하긴 무리이고, 보통 SGD를 사용한다.</p>\n<ul>\n<li>경사하강법 - SGD(Stochastic Gradient Descent)<br>\n모델 전체의 손실함수를 근사한다.<br>\ntraining data에서 랜덤으로 몇 개를 골라서 기울기를 계산하고 업데이트 → 전체 기울기의 불편 추정치<br>\n순서 : 미니배치를 선택 → 미니배치에 대한 기울기 계산 → 파라미터 업데이트 → 성능 좋아질때까지 반복</li>\n<li>Early stopping<br>\n오버피팅(Over fitting)을 방지하는 방지하는데 효율적임.<br>\n검증셋의 Loss 가장 낮은 곳에서 훈련을 멈춤</li>\n<li>적응적 학습률(Adaptive learning rate)<br>\nSGD는 학습률에 민감하기 때문에 이를 보완하기 위한 알고리즘(Adam, Adadelta)가 있다.</li>\n</ul>\n<blockquote>\n<p><a href=\"https://wikidocs.net\" target=\"_blank\" rel=\"nofollow\">위키독스</a>의 <code class=\"language-text\">딥 러닝을 위한 자연어 처리 심화</code> 문서를 참조했고, <a href=\"https://www.boostcourse.org\" target=\"_blank\" rel=\"nofollow\">boostcourse</a>의 <code class=\"language-text\">딥러닝을 이용한 자연어 처리</code> 강의를 보았습니다.</p>\n</blockquote>","frontmatter":{"title":"딥러닝을 이용한 자연어 처리 강의 요약 정리 (3)","summary":"Basic ML - Optimization methods, Backpropagation, Gradient-Based Optimization","date":"2022.01.10.","categories":["딥러닝을 이용한 자연어 처리"]}}}]},"site":{"siteMetadata":{"title":"Jun.Dev","description":"My Blog for record to study","siteUrl":"https://asegh12.github.io"}}},"pageContext":{"slug":"/202201/10/2022-01-10-NLP3/"}},
    "staticQueryHashes": []}