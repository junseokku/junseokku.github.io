{
    "componentChunkName": "component---src-templates-post-template-tsx",
    "path": "/202201/06/2022-01-06-NLP2/",
    "result": {"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"<h2>Loss Function(preview)</h2>\n<h3>Per-example loss function</h3>\n<ul>\n<li>주어진 example에 대해 모델이 얼마나 좋은지 계산한다.</li>\n<li>Training은 Optimization의 과정이며, Training set에 대하여 Loss function이 최대한 작아지도록 한다.</li>\n</ul>\n<h3>다양한 손실함수</h3>\n<ul>\n<li>분류(Classification) : Hinge loss, log-loss 등..</li>\n<li>회귀(Regression) : MSE(Mean Square Error), Robust loss 등..</li>\n<li>Pytorch의 tutorial을 하면서 손실함수가 회귀는 MSE, 분류는 NLL으로 가이드가 있었는데, 어떤 상황에서 왜 이런 손실함수가 사용되어야 하는지 의문을 가질 필요가 있다.</li>\n</ul>\n<h2>Probability in 5 minutes</h2>\n<h3>Event Set, Random Variable, 그리고 Probability</h3>\n<ul>\n<li>사건 집합(Event set)은 오메가(<strong>Ω</strong>)이며 무한히 크다면 continous, 유한하다면 discrete</li>\n<li>확률변수(Random variable)은 x이며 event set의 어떤 event라도 될 수 있다.</li>\n<li>확률(Probability)은 x가 어떤 사건(event)인가를 지정한다.</li>\n<li>확률(probability)은 확률변수(random variable)가 어떤 사건(event)인지 결정해준다.</li>\n</ul>\n<h3>확률(Probability)의 두 가지 성질</h3>\n<ul>\n<li>non-negative : 음수 확률은 존재할 수 없다.</li>\n<li>모든 사건 확률의 합은 1이다. : 이것을 기반으로 손실함수를 정의하게 된다.</li>\n</ul>\n<h3>다양한 확률</h3>\n<ul>\n<li>확률변수가 여러개 있을 수 있다.</li>\n<li>결합확률(joint probability) : 동시에 일어날 확률</li>\n<li>조건부확률(conditional probability)\n<ul>\n<li>주어진 사건이 일어난 것을 가정하고 다른 한 사건이 일어날 확률</li>\n<li>결합확률에서 한계확률을 나눠줌</li>\n<li>식의 변형으로 결합확률은 한계확률과 조건부확률의 곱으로 표현할 수 있다.</li>\n</ul>\n</li>\n<li>Marginalization\n<ol>\n<li>동전 두 개를 던지는데, 이 동전 두 개가 독립적이지 않다고 해보자.</li>\n<li>두번째 동전에서 뒷면이 나올 확률값을 알고싶다.</li>\n<li>두번째 동전이 뒷면이 나온 경우에서, 첫번째 동전의 케이스를 모두 더하며 계산하게된다.</li>\n<li>위 예시를 Marginalization이라고 하며, 여러 확률변수로 구성된 조합확률분포(joint distribution)에서 한 가지 변수에 대한 확률값을 알기 위해, 나머지 변수를 모두 적분하여 제거하는 과정이다.</li>\n</ol>\n</li>\n</ul>\n<h2>Loss function</h2>\n<h3>발상의 전환 : 지도 학습을 확률 문제로</h3>\n<ul>\n<li>\n<p>지도학습 상기 : input을 넣었을 때, output을 산출하는 것</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 85px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/82d0e3a91f70cc131b946a93b16eb55c/30b46/SL.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 38.82352941176471%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABHUlEQVQoz3WScW+CQAzF+f7faP8sS5YsWbLJdDLngqIoME/0GDAZwvFbPPVk6po0afp6r6/XWgBN02g/j6G5wEzNPjD4HmqwTo/hT6wUSl0SmwZH0pbt8tYuKHOJ6810ssgSYpmaoqUQbGsu7Kgo8D1cz0cdkpqw83CHPRzrwvdBH5nlrEXEPBTMfA8/FBory1J7VdVG3TKY8NjpUh8m0YTPts10MsX5GNLp9ijylPvbG169kEUQMPL2zfyphztyiURslG6kYJluTiMnkY/df8Nxejy92Liuy+ciYtB3SIuSuT8mFGv9QMo18SomzXKjMJUrkqxNuIqRaUa13Y1SUW1/yL4Ls4yvRFI3XP3Eawu1+MeUUlfPx3iLqH1Ov+GzbKC28vm7AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/82d0e3a91f70cc131b946a93b16eb55c/38b44/SL.webp 85w\"\n              sizes=\"(max-width: 85px) 100vw, 85px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/82d0e3a91f70cc131b946a93b16eb55c/30b46/SL.png 85w\"\n            sizes=\"(max-width: 85px) 100vw, 85px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/82d0e3a91f70cc131b946a93b16eb55c/30b46/SL.png\"\n            alt=\"SL\"\n            title=\"SL\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n</li>\n<li>\n<p>input으로 output이 특정 y일 확률값을 구하는 것(조건부확률)으로 생각해보자.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 134px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8110fe80cd322a59336f66bfe0fbd7bd/d2ab4/P.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 17.16417910447761%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAmUlEQVQI122M0Q7BMABF/f+vWVgmSMaIhlo7G5Nqi61HVpHswXm55+HkTuiezOYp+SZnfxCUqsI7y61t4xb5mv2xREvBptghjies9VyvDc4/GdgVW/zrHX1i2obFcsX5LMmyFKlrrLmjLzXOGpJkinEeJQVSaXRV4b1HqZKHdfHEOkcI4XvY930UQqDrOsYM0dCFXwN/m7F/AH6P5g08BKwkAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/8110fe80cd322a59336f66bfe0fbd7bd/208f3/P.webp 134w\"\n              sizes=\"(max-width: 134px) 100vw, 134px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/8110fe80cd322a59336f66bfe0fbd7bd/d2ab4/P.png 134w\"\n            sizes=\"(max-width: 134px) 100vw, 134px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/8110fe80cd322a59336f66bfe0fbd7bd/d2ab4/P.png\"\n            alt=\"P\"\n            title=\"P\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n</li>\n<li>\n<p>기존 발상 : 확률이 1인 특정 y를 찾는다 → 새로운 발상 : event set에서 가장 likely한 것</p>\n</li>\n<li>\n<p>어떤 문제인지(분류 혹은 회귀 등)에 따라 확률분포를 정함.(확률분포 선택의 예시)</p>\n<ul>\n<li>이진 분류(Binary Classification) → 베르누이(Bernoulli) 분포</li>\n<li>다중 분류(Muticlass Classification) → 카테고리(Categorical) 분포 : softmax</li>\n<li>선형 회귀(Linear Regression) → 가우시안(Gaussian) 분포</li>\n<li>다항 회귀(Multimodal Linear Regression) → 가우시안 믹스쳐(Mixture of Gaussians)</li>\n</ul>\n</li>\n</ul>\n<h3>손실함수 - Negative Log-Probability</h3>\n<ul>\n<li>NN(DAG)의 output이 확률이면, 자연스럽게 손실함수를 정할 수 있는 방법이 있다.</li>\n<li>모델이 예측한 확률 값을 직접적으로 반영하여 평가</li>\n<li>확률 값을 음의 log함수에 넣어 변환 시킴 -> 잘못 예측할수록, 패널티를 부여하기 위함(직관적으로 해석되며, 불행지수(?)같은 의미)</li>\n</ul>\n<blockquote>\n<p><a href=\"https://wikidocs.net\" target=\"_blank\" rel=\"nofollow\">위키독스</a>의 <code class=\"language-text\">딥 러닝을 위한 자연어 처리 심화</code> 문서를 참조했고, <a href=\"https://www.boostcourse.org\" target=\"_blank\" rel=\"nofollow\">boostcourse</a>의 <code class=\"language-text\">딥러닝을 이용한 자연어 처리</code> 강의를 보았습니다.</p>\n</blockquote>","frontmatter":{"title":"딥러닝을 이용한 자연어 처리 강의 요약 정리 (2)","summary":"Basic ML - Loss Function, Per-example loss function","date":"2022.01.06.","categories":["딥러닝을 이용한 자연어 처리"]}}}]},"site":{"siteMetadata":{"title":"Jun.Dev","description":"My Blog for record to study","siteUrl":"https://junseokku.github.io"}}},"pageContext":{"slug":"/202201/06/2022-01-06-NLP2/"}},
    "staticQueryHashes": []}