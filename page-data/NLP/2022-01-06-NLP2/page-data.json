{
    "componentChunkName": "component---src-templates-post-template-tsx",
    "path": "/NLP/2022-01-06-NLP2/",
    "result": {"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"<h2>Loss Function(preview)</h2>\n<h3>Per-example loss function</h3>\n<ul>\n<li>주어진 example에 대해 모델이 얼마나 좋은지 계산한다.</li>\n<li>Training은 Optimization의 과정이며, Training set에 대하여 Loss function이 최대한 작아지도록 한다.</li>\n</ul>\n<h3>다양한 손실함수</h3>\n<ul>\n<li>분류(Classification) : Hinge loss, log-loss 등..</li>\n<li>회귀(Regression) : MSE(Mean Square Error), Robust loss 등..</li>\n<li>Pytorch의 tutorial을 하면서 손실함수가 회귀는 MSE, 분류는 NLL으로 가이드가 있었는데, 어떤 상황에서 왜 이런 손실함수가 사용되어야 하는지 의문을 가질 필요가 있다.</li>\n</ul>\n<h2>Probability in 5 minutes</h2>\n<h3>Event Set, Random Variable, 그리고 Probability</h3>\n<ul>\n<li>사건 집합(Event set)은 오메가(<strong>Ω</strong>)이며 무한히 크다면 continous, 유한하다면 discrete</li>\n<li>확률변수(Random variable)은 x이며 event set의 어떤 event라도 될 수 있다.</li>\n<li>확률(Probability)은 x가 어떤 사건(event)인가를 지정한다.</li>\n<li>확률(probability)은 확률변수(random variable)가 어떤 사건(event)인지 결정해준다.</li>\n</ul>\n<h3>확률(Probability)의 두 가지 성질</h3>\n<ul>\n<li>non-negative : 음수 확률은 존재할 수 없다.</li>\n<li>모든 사건 확률의 합은 1이다. : 이것을 기반으로 손실함수를 정의하게 된다.</li>\n</ul>\n<h3>다양한 확률</h3>\n<ul>\n<li>확률변수가 여러개 있을 수 있다.</li>\n<li>결합확률(joint probability) : 동시에 일어날 확률</li>\n<li>조건부확률(conditional probability)\n<ul>\n<li>\n<p>주어진 사건이 일어난 것을 가정하고 다른 한 사건이 일어날 확률</p>\n</li>\n<li>\n<p>결합확률에서 한계확률을 나눠줌</p>\n</li>\n<li>\n<p>식의 변형으로 결합확률은 한계확률과 조건부확률의 곱으로 표현할 수 있다.</p>\n</li>\n</ul>\n</li>\n<li>Marginalization\n<ol>\n<li>동전 두 개를 던지는데, 이 동전 두 개가 독립적이지 않다고 해보자.</li>\n<li>두번째 동전에서 뒷면이 나올 확률값을 알고싶다.</li>\n<li>두번째 동전이 뒷면이 나온 경우에서, 첫번째 동전의 케이스를 모두 더하며 계산하게된다.</li>\n<li>위 예시를 Marginalization이라고 하며, 여러 확률변수로 구성된 조합확률분포(joint distribution)에서 한 가지 변수에 대한 확률값을 알기 위해, 나머지 변수를 모두 적분하여 제거하는 과정이다.</li>\n</ol>\n</li>\n</ul>\n<h2>Loss function</h2>\n<h3>발상의 전환 : 지도 학습을 확률 문제로</h3>\n<ul>\n<li>\n<p>지도학습 상기 : input을 넣었을 때, output을 산출하는 것</p>\n  <img src = \"/assets/images/SL.png\"> \n</li>\n<li>\n<p>input으로 output이 특정 y일 확률값을 구하는 것(조건부확률)으로 생각해보자.</p>\n  <img src = \"/assets/images/P.png\"> \n</li>\n<li>\n<p>기존 발상 : 확률이 1인 특정 y를 찾는다 → 새로운 발상 : event set에서 가장 likely한 것</p>\n</li>\n<li>\n<p>어떤 문제인지(분류 혹은 회귀 등)에 따라 확률분포를 정함.(확률분포 선택의 예시)</p>\n<ul>\n<li>이진 분류(Binary Classification) → 베르누이(Bernoulli) 분포</li>\n<li>다중 분류(Muticlass Classification) → 카테고리(Categorical) 분포 : softmax</li>\n<li>선형 회귀(Linear Regression) → 가우시안(Gaussian) 분포</li>\n<li>다항 회귀(Multimodal Linear Regression) → 가우시안 믹스쳐(Mixture of Gaussians)</li>\n</ul>\n</li>\n</ul>\n<h3>손실함수 - Negative Log-Probability</h3>\n<ul>\n<li>NN(DAG)의 output이 확률이면, 자연스럽게 손실함수를 정할 수 있는 방법이 있다.</li>\n<li>모델이 예측한 확률 값을 직접적으로 반영하여 평가</li>\n<li>확률 값을 음의 log함수에 넣어 변환 시킴 -> 잘못 예측할수록, 패널티를 부여하기 위함(직관적으로 해석되며, 불행지수(?)같은 의미)</li>\n</ul>\n<blockquote>\n<p><a href=\"https://wikidocs.net\" target=\"_blank\" rel=\"nofollow\">위키독스</a>의 <code class=\"language-text\">딥 러닝을 위한 자연어 처리 심화</code> 문서를 참조했고, <a href=\"https://www.boostcourse.org\" target=\"_blank\" rel=\"nofollow\">boostcourse</a>의 <code class=\"language-text\">딥러닝을 이용한 자연어 처리</code> 강의를 보았습니다.</p>\n</blockquote>","frontmatter":{"title":"딥러닝을 이용한 자연어 처리 강의 요약 정리 (2)","summary":"Basic ML - Loss Function, Per-example loss function","date":null,"categories":["딥러닝을 이용한 자연어 처리"],"thumbnail":null},"tableOfContents":"<ul>\n<li>\n<p><a href=\"#loss-functionpreview\">Loss Function(preview)</a></p>\n<ul>\n<li><a href=\"#per-example-loss-function\">Per-example loss function</a></li>\n<li><a href=\"#%EB%8B%A4%EC%96%91%ED%95%9C-%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98\">다양한 손실함수</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#probability-in-5-minutes\">Probability in 5 minutes</a></p>\n<ul>\n<li><a href=\"#event-set-random-variable-%EA%B7%B8%EB%A6%AC%EA%B3%A0-probability\">Event Set, Random Variable, 그리고 Probability</a></li>\n<li><a href=\"#%ED%99%95%EB%A5%A0probability%EC%9D%98-%EB%91%90-%EA%B0%80%EC%A7%80-%EC%84%B1%EC%A7%88\">확률(Probability)의 두 가지 성질</a></li>\n<li><a href=\"#%EB%8B%A4%EC%96%91%ED%95%9C-%ED%99%95%EB%A5%A0\">다양한 확률</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#loss-function\">Loss function</a></p>\n<ul>\n<li><a href=\"#%EB%B0%9C%EC%83%81%EC%9D%98-%EC%A0%84%ED%99%98--%EC%A7%80%EB%8F%84-%ED%95%99%EC%8A%B5%EC%9D%84-%ED%99%95%EB%A5%A0-%EB%AC%B8%EC%A0%9C%EB%A1%9C\">발상의 전환 : 지도 학습을 확률 문제로</a></li>\n<li><a href=\"#%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98---negative-log-probability\">손실함수 - Negative Log-Probability</a></li>\n</ul>\n</li>\n</ul>"}}]}},"pageContext":{"slug":"/NLP/2022-01-06-NLP2/"}},
    "staticQueryHashes": ["3649515864"]}