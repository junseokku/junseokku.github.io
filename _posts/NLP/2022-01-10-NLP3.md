---
title: 딥러닝을 이용한 자연어 처리 강의 요약 정리 (3)
excerpt: "Basic ML - Optimization methods, Backpropagation, Gradient-Based Optimization"

categories:
  - 딥러닝을 이용한 자연어 처리

tags:
  - ["NLP", "자연어처리", "딥러닝", "최적화", "역전파"]
---  

### Optimization methods

- 손실을 최소화하는 최적화 방법을 배우기 위해 알아야 하는 것 두 가지. 
1. 최적화 방법 선택하기
2. 파라미터를 평가하기 위한 최적화 알고리즘 사용 방법.

- Random guided search  
    idea : 랜덤하게 파라미터를 수정하면서 최소화 하는 방향으로 간다.  
    어떤 NN이나 손실함수에도 적용하기 쉽지만, 고차원 파라미터 공간에서는 차원의 저주(curse of dimensionality)로 비효율적이다.
    

- Gradient Based Algorithm  
    이 알고리즘은 손실함수가 연속이고(continuous) 미분가능한(differentiable) 성질을 갖고 있어야 한다.  
    idea : 가장 먼저 파라미터를 랜덤픽하고, 손실함수를 미분하여 기울기를 구한다. 기울기 값을 파라미터 값에서 빼주면, 손실함수가 최소가되는 파라미터값에 가까워진다.  
    단, 기울기 값을 파라미터 값에서 뺄 때, 학습률(learning rate)를 고려하여 빼준다. 학습률이 낮으면 오랜 시간이 걸리고, 학습률이 크면 변동 폭이 커서 파라미터의 값을 제대로 찾지 못하게 된다.
    

---

### Backpropagation

- 자동 미분법(Automatic differentiation)  
    규모가 굉장히 큰 방정식의 기울기를 어떻게 구할 것인가에 대한 문제 → 자동 미분법 
    모델을 연결하는 DAG는 미분 가능한 함수들로 구성되어 있으며, 미분의 연쇄법칙으로 자동 미분됨.
    
- Backward Computation - 역전파(Backpropagation)  
    과정 : 각 노드에서 Jacobian-vector product 계산 → DAG 역순으로 전파  
    장점 : 기울기를 직접 계산할 필요가 없으며, 모델을 만드는데 집중할 수 있다.
    

---

### Gradient-Based Optimization

우리가 역전파 알고리즘으로 패키지에서 사용하는 것은 Gradient descent, L-BFGS, Conjugate gradient가 있는데, 매개변수가 많을수록 시간이 오래걸린다. 이유는 훈련 샘플 전체의 Loss는 각 샘플에 대한 Loss 합이며 데이터가 많아질수록 시간이 오래걸리기 때문이다. 따라서 off-the shelf gradient-based optimization을 사용하긴 무리이고, 보통 SGD를 사용한다.

- 경사하강법 - SGD(Stochastic Gradient Descent)  
    모델 전체의 손실함수를 근사한다.  
    training data에서 랜덤으로 몇 개를 골라서 기울기를 계산하고 업데이트 → 전체 기울기의 불편 추정치  
    순서 : 미니배치를 선택 → 미니배치에 대한 기울기 계산 → 파라미터 업데이트 → 성능 좋아질때까지 반복
    
- Early stopping  
    오버피팅(Over fitting)을 방지하는 방지하는데 효율적임.  
    검증셋의 Loss 가장 낮은 곳에서 훈련을 멈춤
    
- 적응적 학습률(Adaptive learning rate)  
    SGD는 학습률에 민감하기 때문에 이를 보완하기 위한 알고리즘(Adam, Adadelta)가 있다.

> [위키독스](wikidocs.net)의 `딥 러닝을 위한 자연어 처리 심화` 문서를 참조했고, [boostcourse](boostcourse.org)의 `딥러닝을 이용한 자연어 처리` 강의를 보았습니다.