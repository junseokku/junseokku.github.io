---
title: 딥러닝을 이용한 자연어 처리 강의 요약 정리 (2)
summary: 'Basic ML - Loss Function, Per-example loss function'

categories:
  - 딥러닝을 이용한 자연어 처리

tags:
  - ['NLP', '자연어처리', '딥러닝', '손실함수', 'loss function']
date: 2022-01-06
---

## Loss Function(preview)

### Per-example loss function

- 주어진 example에 대해 모델이 얼마나 좋은지 계산한다.
- Training은 Optimization의 과정이며, Training set에 대하여 Loss function이 최대한 작아지도록 한다.

### 다양한 손실함수

- 분류(Classification) : Hinge loss, log-loss 등..
- 회귀(Regression) : MSE(Mean Square Error), Robust loss 등..
- Pytorch의 tutorial을 하면서 손실함수가 회귀는 MSE, 분류는 NLL으로 가이드가 있었는데, 어떤 상황에서 왜 이런 손실함수가 사용되어야 하는지 의문을 가질 필요가 있다.

## Probability in 5 minutes

### Event Set, Random Variable, 그리고 Probability

- 사건 집합(Event set)은 오메가(**Ω**)이며 무한히 크다면 continous, 유한하다면 discrete
- 확률변수(Random variable)은 x이며 event set의 어떤 event라도 될 수 있다.
- 확률(Probability)은 x가 어떤 사건(event)인가를 지정한다.
- 확률(probability)은 확률변수(random variable)가 어떤 사건(event)인지 결정해준다.

### 확률(Probability)의 두 가지 성질

- non-negative : 음수 확률은 존재할 수 없다.
- 모든 사건 확률의 합은 1이다. : 이것을 기반으로 손실함수를 정의하게 된다.

### 다양한 확률

- 확률변수가 여러개 있을 수 있다.
- 결합확률(joint probability) : 동시에 일어날 확률
- 조건부확률(conditional probability)
  - 주어진 사건이 일어난 것을 가정하고 다른 한 사건이 일어날 확률
  - 결합확률에서 한계확률을 나눠줌
  - 식의 변형으로 결합확률은 한계확률과 조건부확률의 곱으로 표현할 수 있다.
- Marginalization
  1. 동전 두 개를 던지는데, 이 동전 두 개가 독립적이지 않다고 해보자.
  2. 두번째 동전에서 뒷면이 나올 확률값을 알고싶다.
  3. 두번째 동전이 뒷면이 나온 경우에서, 첫번째 동전의 케이스를 모두 더하며 계산하게된다.
  4. 위 예시를 Marginalization이라고 하며, 여러 확률변수로 구성된 조합확률분포(joint distribution)에서 한 가지 변수에 대한 확률값을 알기 위해, 나머지 변수를 모두 적분하여 제거하는 과정이다.

## Loss function

### 발상의 전환 : 지도 학습을 확률 문제로

- 지도학습 상기 : input을 넣었을 때, output을 산출하는 것

    ![SL](./assets/SL.png)

- input으로 output이 특정 y일 확률값을 구하는 것(조건부확률)으로 생각해보자.

    ![P](./assets/P.png)

- 기존 발상 : 확률이 1인 특정 y를 찾는다 → 새로운 발상 : event set에서 가장 likely한 것
- 어떤 문제인지(분류 혹은 회귀 등)에 따라 확률분포를 정함.(확률분포 선택의 예시)
  - 이진 분류(Binary Classification) → 베르누이(Bernoulli) 분포
  - 다중 분류(Muticlass Classification) → 카테고리(Categorical) 분포 : softmax
  - 선형 회귀(Linear Regression) → 가우시안(Gaussian) 분포
  - 다항 회귀(Multimodal Linear Regression) → 가우시안 믹스쳐(Mixture of Gaussians)

### 손실함수 - Negative Log-Probability

- NN(DAG)의 output이 확률이면, 자연스럽게 손실함수를 정할 수 있는 방법이 있다.
- 모델이 예측한 확률 값을 직접적으로 반영하여 평가
- 확률 값을 음의 log함수에 넣어 변환 시킴 -> 잘못 예측할수록, 패널티를 부여하기 위함(직관적으로 해석되며, 불행지수(?)같은 의미)

> [위키독스](https://wikidocs.net)의 `딥 러닝을 위한 자연어 처리 심화` 문서를 참조했고, [boostcourse](https://www.boostcourse.org)의 `딥러닝을 이용한 자연어 처리` 강의를 보았습니다.
